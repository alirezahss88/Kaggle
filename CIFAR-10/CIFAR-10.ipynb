{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d103cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, BatchNormalization, Conv2D, Dropout, Activation, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd711ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e935bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18584f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "x_train = (x_train - mean)/std\n",
    "x_test = (x_test - mean)/std\n",
    "x_vaild = (x_valid - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d75aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)\n",
    "y_valid = to_categorical(y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe561a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=15,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=False)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2677aaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4), input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4), input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b674b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint('model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30c0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, \n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be13202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "352/352 [==============================] - 182s 512ms/step - loss: 2.0241 - accuracy: 0.3893 - val_loss: 1.8077 - val_accuracy: 0.4076\n",
      "Epoch 2/125\n",
      "352/352 [==============================] - 189s 538ms/step - loss: 1.4672 - accuracy: 0.5225 - val_loss: 1.4274 - val_accuracy: 0.5500\n",
      "Epoch 3/125\n",
      "352/352 [==============================] - 207s 587ms/step - loss: 1.2556 - accuracy: 0.5882 - val_loss: 1.0394 - val_accuracy: 0.6668\n",
      "Epoch 4/125\n",
      "352/352 [==============================] - 209s 592ms/step - loss: 1.1081 - accuracy: 0.6378 - val_loss: 1.2035 - val_accuracy: 0.6350\n",
      "Epoch 5/125\n",
      "352/352 [==============================] - 208s 591ms/step - loss: 1.0131 - accuracy: 0.6694 - val_loss: 0.9944 - val_accuracy: 0.6856\n",
      "Epoch 6/125\n",
      "352/352 [==============================] - 200s 568ms/step - loss: 0.9343 - accuracy: 0.6984 - val_loss: 0.7890 - val_accuracy: 0.7464\n",
      "Epoch 7/125\n",
      "352/352 [==============================] - 202s 573ms/step - loss: 0.8862 - accuracy: 0.7144 - val_loss: 0.7616 - val_accuracy: 0.7602\n",
      "Epoch 8/125\n",
      "352/352 [==============================] - 199s 564ms/step - loss: 0.8363 - accuracy: 0.7290 - val_loss: 0.7509 - val_accuracy: 0.7706\n",
      "Epoch 9/125\n",
      "352/352 [==============================] - 203s 576ms/step - loss: 0.8101 - accuracy: 0.7411 - val_loss: 0.7274 - val_accuracy: 0.7738\n",
      "Epoch 10/125\n",
      "352/352 [==============================] - 199s 565ms/step - loss: 0.7674 - accuracy: 0.7548 - val_loss: 0.7337 - val_accuracy: 0.7698\n",
      "Epoch 11/125\n",
      "352/352 [==============================] - 191s 543ms/step - loss: 0.7543 - accuracy: 0.7586 - val_loss: 0.7807 - val_accuracy: 0.7634\n",
      "Epoch 12/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.7285 - accuracy: 0.7719 - val_loss: 0.6841 - val_accuracy: 0.7844\n",
      "Epoch 13/125\n",
      "352/352 [==============================] - 192s 544ms/step - loss: 0.7070 - accuracy: 0.7808 - val_loss: 0.6161 - val_accuracy: 0.8112\n",
      "Epoch 14/125\n",
      "352/352 [==============================] - 206s 584ms/step - loss: 0.6977 - accuracy: 0.7864 - val_loss: 0.6204 - val_accuracy: 0.8214\n",
      "Epoch 15/125\n",
      "352/352 [==============================] - 207s 587ms/step - loss: 0.6802 - accuracy: 0.7890 - val_loss: 0.6370 - val_accuracy: 0.8210\n",
      "Epoch 16/125\n",
      "352/352 [==============================] - 193s 549ms/step - loss: 0.6757 - accuracy: 0.7952 - val_loss: 0.6332 - val_accuracy: 0.8202\n",
      "Epoch 17/125\n",
      "352/352 [==============================] - 183s 520ms/step - loss: 0.6617 - accuracy: 0.8015 - val_loss: 0.5737 - val_accuracy: 0.8346\n",
      "Epoch 18/125\n",
      "352/352 [==============================] - 183s 518ms/step - loss: 0.6497 - accuracy: 0.8074 - val_loss: 0.6402 - val_accuracy: 0.8168\n",
      "Epoch 19/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.6466 - accuracy: 0.8077 - val_loss: 0.6285 - val_accuracy: 0.8224\n",
      "Epoch 20/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.6353 - accuracy: 0.8136 - val_loss: 0.5724 - val_accuracy: 0.8386\n",
      "Epoch 21/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.6290 - accuracy: 0.8166 - val_loss: 0.6010 - val_accuracy: 0.8336\n",
      "Epoch 22/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.6253 - accuracy: 0.8190 - val_loss: 0.5507 - val_accuracy: 0.8466\n",
      "Epoch 23/125\n",
      "352/352 [==============================] - 180s 511ms/step - loss: 0.6220 - accuracy: 0.8180 - val_loss: 0.5780 - val_accuracy: 0.8394\n",
      "Epoch 24/125\n",
      "352/352 [==============================] - 180s 512ms/step - loss: 0.6139 - accuracy: 0.8252 - val_loss: 0.6263 - val_accuracy: 0.8286\n",
      "Epoch 25/125\n",
      "352/352 [==============================] - 184s 524ms/step - loss: 0.6121 - accuracy: 0.8249 - val_loss: 0.5870 - val_accuracy: 0.8428\n",
      "Epoch 26/125\n",
      "352/352 [==============================] - 208s 591ms/step - loss: 0.6025 - accuracy: 0.8312 - val_loss: 0.6219 - val_accuracy: 0.8296\n",
      "Epoch 27/125\n",
      "352/352 [==============================] - 206s 586ms/step - loss: 0.6006 - accuracy: 0.8298 - val_loss: 0.5409 - val_accuracy: 0.8542\n",
      "Epoch 28/125\n",
      "352/352 [==============================] - 198s 561ms/step - loss: 0.5893 - accuracy: 0.8347 - val_loss: 0.5531 - val_accuracy: 0.8558\n",
      "Epoch 29/125\n",
      "352/352 [==============================] - 193s 549ms/step - loss: 0.5956 - accuracy: 0.8338 - val_loss: 0.5562 - val_accuracy: 0.8478\n",
      "Epoch 30/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5899 - accuracy: 0.8360 - val_loss: 0.5384 - val_accuracy: 0.8592\n",
      "Epoch 31/125\n",
      "352/352 [==============================] - 181s 513ms/step - loss: 0.5851 - accuracy: 0.8372 - val_loss: 0.5389 - val_accuracy: 0.8570\n",
      "Epoch 32/125\n",
      "352/352 [==============================] - 179s 509ms/step - loss: 0.5869 - accuracy: 0.8388 - val_loss: 0.5894 - val_accuracy: 0.8430\n",
      "Epoch 33/125\n",
      "352/352 [==============================] - 180s 511ms/step - loss: 0.5836 - accuracy: 0.8378 - val_loss: 0.5645 - val_accuracy: 0.8514\n",
      "Epoch 34/125\n",
      "352/352 [==============================] - 179s 508ms/step - loss: 0.5775 - accuracy: 0.8425 - val_loss: 0.5844 - val_accuracy: 0.8468\n",
      "Epoch 35/125\n",
      "352/352 [==============================] - 181s 513ms/step - loss: 0.5774 - accuracy: 0.8413 - val_loss: 0.5503 - val_accuracy: 0.8554\n",
      "Epoch 36/125\n",
      "352/352 [==============================] - 179s 509ms/step - loss: 0.5786 - accuracy: 0.8414 - val_loss: 0.5488 - val_accuracy: 0.8590\n",
      "Epoch 37/125\n",
      "352/352 [==============================] - 179s 507ms/step - loss: 0.5697 - accuracy: 0.8453 - val_loss: 0.5508 - val_accuracy: 0.8542\n",
      "Epoch 38/125\n",
      "352/352 [==============================] - 180s 512ms/step - loss: 0.5673 - accuracy: 0.8467 - val_loss: 0.5603 - val_accuracy: 0.8576\n",
      "Epoch 39/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5666 - accuracy: 0.8472 - val_loss: 0.5358 - val_accuracy: 0.8648\n",
      "Epoch 40/125\n",
      "352/352 [==============================] - 179s 508ms/step - loss: 0.5606 - accuracy: 0.8491 - val_loss: 0.5551 - val_accuracy: 0.8560\n",
      "Epoch 41/125\n",
      "352/352 [==============================] - 180s 510ms/step - loss: 0.5593 - accuracy: 0.8494 - val_loss: 0.5242 - val_accuracy: 0.8668\n",
      "Epoch 42/125\n",
      "352/352 [==============================] - 179s 508ms/step - loss: 0.5615 - accuracy: 0.8498 - val_loss: 0.5043 - val_accuracy: 0.8772\n",
      "Epoch 43/125\n",
      "352/352 [==============================] - 179s 508ms/step - loss: 0.5545 - accuracy: 0.8507 - val_loss: 0.5205 - val_accuracy: 0.8706\n",
      "Epoch 44/125\n",
      "352/352 [==============================] - 193s 547ms/step - loss: 0.5592 - accuracy: 0.8507 - val_loss: 0.4935 - val_accuracy: 0.8792\n",
      "Epoch 45/125\n",
      "352/352 [==============================] - 212s 601ms/step - loss: 0.5534 - accuracy: 0.8524 - val_loss: 0.5441 - val_accuracy: 0.8630\n",
      "Epoch 46/125\n",
      "352/352 [==============================] - 184s 524ms/step - loss: 0.5512 - accuracy: 0.8541 - val_loss: 0.5690 - val_accuracy: 0.8558\n",
      "Epoch 47/125\n",
      "352/352 [==============================] - 180s 511ms/step - loss: 0.5495 - accuracy: 0.8547 - val_loss: 0.5457 - val_accuracy: 0.8624\n",
      "Epoch 48/125\n",
      "352/352 [==============================] - 180s 510ms/step - loss: 0.5528 - accuracy: 0.8536 - val_loss: 0.5182 - val_accuracy: 0.8768\n",
      "Epoch 49/125\n",
      "352/352 [==============================] - 179s 510ms/step - loss: 0.5485 - accuracy: 0.8567 - val_loss: 0.5202 - val_accuracy: 0.8726\n",
      "Epoch 50/125\n",
      "352/352 [==============================] - 181s 514ms/step - loss: 0.5463 - accuracy: 0.8557 - val_loss: 0.5324 - val_accuracy: 0.8696\n",
      "Epoch 51/125\n",
      "352/352 [==============================] - 181s 513ms/step - loss: 0.5421 - accuracy: 0.8583 - val_loss: 0.5465 - val_accuracy: 0.8660\n",
      "Epoch 52/125\n",
      "352/352 [==============================] - 179s 510ms/step - loss: 0.5433 - accuracy: 0.8570 - val_loss: 0.5159 - val_accuracy: 0.8788\n",
      "Epoch 53/125\n",
      "352/352 [==============================] - 179s 508ms/step - loss: 0.5408 - accuracy: 0.8601 - val_loss: 0.5580 - val_accuracy: 0.8598\n",
      "Epoch 54/125\n",
      "352/352 [==============================] - 179s 509ms/step - loss: 0.5420 - accuracy: 0.8589 - val_loss: 0.5437 - val_accuracy: 0.8652\n",
      "Epoch 55/125\n",
      "352/352 [==============================] - 187s 531ms/step - loss: 0.5409 - accuracy: 0.8608 - val_loss: 0.5249 - val_accuracy: 0.8696\n",
      "Epoch 56/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 190s 539ms/step - loss: 0.5385 - accuracy: 0.8605 - val_loss: 0.5253 - val_accuracy: 0.8698\n",
      "Epoch 57/125\n",
      "352/352 [==============================] - 187s 532ms/step - loss: 0.5387 - accuracy: 0.8588 - val_loss: 0.5440 - val_accuracy: 0.8636\n",
      "Epoch 58/125\n",
      "352/352 [==============================] - 187s 530ms/step - loss: 0.5363 - accuracy: 0.8609 - val_loss: 0.5607 - val_accuracy: 0.8632\n",
      "Epoch 59/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5342 - accuracy: 0.8626 - val_loss: 0.5195 - val_accuracy: 0.8756\n",
      "Epoch 60/125\n",
      "352/352 [==============================] - 184s 522ms/step - loss: 0.5389 - accuracy: 0.8608 - val_loss: 0.5461 - val_accuracy: 0.8712\n",
      "Epoch 61/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.5325 - accuracy: 0.8634 - val_loss: 0.5143 - val_accuracy: 0.8780\n",
      "Epoch 62/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5337 - accuracy: 0.8637 - val_loss: 0.5114 - val_accuracy: 0.8750\n",
      "Epoch 63/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5332 - accuracy: 0.8627 - val_loss: 0.5112 - val_accuracy: 0.8806\n",
      "Epoch 64/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5302 - accuracy: 0.8643 - val_loss: 0.5498 - val_accuracy: 0.8630\n",
      "Epoch 65/125\n",
      "352/352 [==============================] - 203s 578ms/step - loss: 0.5266 - accuracy: 0.8655 - val_loss: 0.5572 - val_accuracy: 0.8644\n",
      "Epoch 66/125\n",
      "352/352 [==============================] - 179s 509ms/step - loss: 0.5229 - accuracy: 0.8670 - val_loss: 0.5487 - val_accuracy: 0.8670\n",
      "Epoch 67/125\n",
      "352/352 [==============================] - 183s 519ms/step - loss: 0.5264 - accuracy: 0.8643 - val_loss: 0.5479 - val_accuracy: 0.8692\n",
      "Epoch 68/125\n",
      "352/352 [==============================] - 186s 529ms/step - loss: 0.5279 - accuracy: 0.8656 - val_loss: 0.5354 - val_accuracy: 0.8684\n",
      "Epoch 69/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5263 - accuracy: 0.8657 - val_loss: 0.5044 - val_accuracy: 0.8798\n",
      "Epoch 70/125\n",
      "352/352 [==============================] - 181s 514ms/step - loss: 0.5249 - accuracy: 0.8671 - val_loss: 0.5335 - val_accuracy: 0.8732\n",
      "Epoch 71/125\n",
      "352/352 [==============================] - 180s 512ms/step - loss: 0.5250 - accuracy: 0.8664 - val_loss: 0.5209 - val_accuracy: 0.8738\n",
      "Epoch 72/125\n",
      "352/352 [==============================] - 180s 512ms/step - loss: 0.5199 - accuracy: 0.8689 - val_loss: 0.5132 - val_accuracy: 0.8788\n",
      "Epoch 73/125\n",
      "352/352 [==============================] - 180s 511ms/step - loss: 0.5213 - accuracy: 0.8684 - val_loss: 0.5722 - val_accuracy: 0.8620\n",
      "Epoch 74/125\n",
      "352/352 [==============================] - 180s 513ms/step - loss: 0.5226 - accuracy: 0.8651 - val_loss: 0.4987 - val_accuracy: 0.8848\n",
      "Epoch 75/125\n",
      "352/352 [==============================] - 183s 519ms/step - loss: 0.5244 - accuracy: 0.8663 - val_loss: 0.5086 - val_accuracy: 0.8778\n",
      "Epoch 76/125\n",
      "352/352 [==============================] - 187s 531ms/step - loss: 0.5177 - accuracy: 0.8690 - val_loss: 0.5443 - val_accuracy: 0.8666\n",
      "Epoch 77/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5170 - accuracy: 0.8687 - val_loss: 0.5100 - val_accuracy: 0.8814\n",
      "Epoch 78/125\n",
      "352/352 [==============================] - 181s 513ms/step - loss: 0.5206 - accuracy: 0.8678 - val_loss: 0.5211 - val_accuracy: 0.8778\n",
      "Epoch 79/125\n",
      "352/352 [==============================] - 181s 514ms/step - loss: 0.5173 - accuracy: 0.8696 - val_loss: 0.5052 - val_accuracy: 0.8784\n",
      "Epoch 80/125\n",
      "352/352 [==============================] - 181s 513ms/step - loss: 0.5240 - accuracy: 0.8667 - val_loss: 0.4991 - val_accuracy: 0.8818\n",
      "Epoch 81/125\n",
      "352/352 [==============================] - 181s 513ms/step - loss: 0.5154 - accuracy: 0.8714 - val_loss: 0.5034 - val_accuracy: 0.8790\n",
      "Epoch 82/125\n",
      "352/352 [==============================] - 180s 512ms/step - loss: 0.5107 - accuracy: 0.8701 - val_loss: 0.5066 - val_accuracy: 0.8770\n",
      "Epoch 83/125\n",
      "352/352 [==============================] - 181s 514ms/step - loss: 0.5173 - accuracy: 0.8698 - val_loss: 0.5209 - val_accuracy: 0.8742\n",
      "Epoch 84/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5116 - accuracy: 0.8715 - val_loss: 0.5075 - val_accuracy: 0.8790\n",
      "Epoch 85/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.5151 - accuracy: 0.8714 - val_loss: 0.4917 - val_accuracy: 0.8806\n",
      "Epoch 86/125\n",
      "352/352 [==============================] - 181s 515ms/step - loss: 0.5160 - accuracy: 0.8697 - val_loss: 0.4964 - val_accuracy: 0.8850\n",
      "Epoch 87/125\n",
      "352/352 [==============================] - 181s 514ms/step - loss: 0.5097 - accuracy: 0.8730 - val_loss: 0.5660 - val_accuracy: 0.8636\n",
      "Epoch 88/125\n",
      "352/352 [==============================] - 185s 525ms/step - loss: 0.5057 - accuracy: 0.8715 - val_loss: 0.5515 - val_accuracy: 0.8710\n",
      "Epoch 89/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5102 - accuracy: 0.8714 - val_loss: 0.5233 - val_accuracy: 0.8772\n",
      "Epoch 90/125\n",
      "352/352 [==============================] - 181s 515ms/step - loss: 0.5126 - accuracy: 0.8710 - val_loss: 0.5338 - val_accuracy: 0.8778\n",
      "Epoch 91/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5078 - accuracy: 0.8729 - val_loss: 0.5117 - val_accuracy: 0.8776\n",
      "Epoch 92/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5034 - accuracy: 0.8742 - val_loss: 0.5883 - val_accuracy: 0.8624\n",
      "Epoch 93/125\n",
      "352/352 [==============================] - 183s 519ms/step - loss: 0.5079 - accuracy: 0.8728 - val_loss: 0.5049 - val_accuracy: 0.8876\n",
      "Epoch 94/125\n",
      "352/352 [==============================] - 183s 519ms/step - loss: 0.5128 - accuracy: 0.8723 - val_loss: 0.5177 - val_accuracy: 0.8778\n",
      "Epoch 95/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5048 - accuracy: 0.8738 - val_loss: 0.5176 - val_accuracy: 0.8724\n",
      "Epoch 96/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5119 - accuracy: 0.8719 - val_loss: 0.5127 - val_accuracy: 0.8772\n",
      "Epoch 97/125\n",
      "352/352 [==============================] - 184s 521ms/step - loss: 0.5075 - accuracy: 0.8717 - val_loss: 0.4957 - val_accuracy: 0.8836\n",
      "Epoch 98/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.5045 - accuracy: 0.8718 - val_loss: 0.5157 - val_accuracy: 0.8748\n",
      "Epoch 99/125\n",
      "352/352 [==============================] - 184s 522ms/step - loss: 0.5069 - accuracy: 0.8744 - val_loss: 0.5038 - val_accuracy: 0.8832\n",
      "Epoch 100/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5066 - accuracy: 0.8732 - val_loss: 0.5246 - val_accuracy: 0.8716\n",
      "Epoch 101/125\n",
      "352/352 [==============================] - 182s 518ms/step - loss: 0.5052 - accuracy: 0.8749 - val_loss: 0.4943 - val_accuracy: 0.8854\n",
      "Epoch 102/125\n",
      "352/352 [==============================] - 183s 520ms/step - loss: 0.5064 - accuracy: 0.8726 - val_loss: 0.4787 - val_accuracy: 0.8848\n",
      "Epoch 103/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5015 - accuracy: 0.8756 - val_loss: 0.5211 - val_accuracy: 0.8792\n",
      "Epoch 104/125\n",
      "352/352 [==============================] - 183s 520ms/step - loss: 0.4995 - accuracy: 0.8767 - val_loss: 0.5011 - val_accuracy: 0.8818\n",
      "Epoch 105/125\n",
      "352/352 [==============================] - 183s 521ms/step - loss: 0.5043 - accuracy: 0.8743 - val_loss: 0.4731 - val_accuracy: 0.8926\n",
      "Epoch 106/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5007 - accuracy: 0.8759 - val_loss: 0.5110 - val_accuracy: 0.8794\n",
      "Epoch 107/125\n",
      "352/352 [==============================] - 183s 518ms/step - loss: 0.5008 - accuracy: 0.8760 - val_loss: 0.5366 - val_accuracy: 0.8734\n",
      "Epoch 108/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.5010 - accuracy: 0.8758 - val_loss: 0.5179 - val_accuracy: 0.8716\n",
      "Epoch 109/125\n",
      "352/352 [==============================] - 183s 520ms/step - loss: 0.5036 - accuracy: 0.8741 - val_loss: 0.4999 - val_accuracy: 0.8788\n",
      "Epoch 110/125\n",
      "352/352 [==============================] - 183s 521ms/step - loss: 0.4990 - accuracy: 0.8777 - val_loss: 0.5028 - val_accuracy: 0.8830\n",
      "Epoch 111/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 183s 520ms/step - loss: 0.5021 - accuracy: 0.8762 - val_loss: 0.4918 - val_accuracy: 0.8824\n",
      "Epoch 112/125\n",
      "352/352 [==============================] - 184s 523ms/step - loss: 0.4996 - accuracy: 0.8775 - val_loss: 0.4963 - val_accuracy: 0.8838\n",
      "Epoch 113/125\n",
      "352/352 [==============================] - 183s 520ms/step - loss: 0.4978 - accuracy: 0.8764 - val_loss: 0.4759 - val_accuracy: 0.8884\n",
      "Epoch 114/125\n",
      "352/352 [==============================] - 184s 524ms/step - loss: 0.4936 - accuracy: 0.8784 - val_loss: 0.5151 - val_accuracy: 0.8772\n",
      "Epoch 115/125\n",
      "352/352 [==============================] - 185s 524ms/step - loss: 0.4983 - accuracy: 0.8771 - val_loss: 0.4872 - val_accuracy: 0.8858\n",
      "Epoch 116/125\n",
      "352/352 [==============================] - 184s 523ms/step - loss: 0.4997 - accuracy: 0.8773 - val_loss: 0.5359 - val_accuracy: 0.8738\n",
      "Epoch 117/125\n",
      "352/352 [==============================] - 183s 519ms/step - loss: 0.4956 - accuracy: 0.8765 - val_loss: 0.5206 - val_accuracy: 0.8816\n",
      "Epoch 118/125\n",
      "352/352 [==============================] - 184s 522ms/step - loss: 0.4983 - accuracy: 0.8779 - val_loss: 0.4952 - val_accuracy: 0.8858\n",
      "Epoch 119/125\n",
      "352/352 [==============================] - 183s 519ms/step - loss: 0.4958 - accuracy: 0.8780 - val_loss: 0.5491 - val_accuracy: 0.8710\n",
      "Epoch 120/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.4967 - accuracy: 0.8772 - val_loss: 0.5325 - val_accuracy: 0.8734\n",
      "Epoch 121/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.4969 - accuracy: 0.8796 - val_loss: 0.5372 - val_accuracy: 0.8736\n",
      "Epoch 122/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.4994 - accuracy: 0.8764 - val_loss: 0.4945 - val_accuracy: 0.8862\n",
      "Epoch 123/125\n",
      "352/352 [==============================] - 181s 515ms/step - loss: 0.4898 - accuracy: 0.8787 - val_loss: 0.4948 - val_accuracy: 0.8830\n",
      "Epoch 124/125\n",
      "352/352 [==============================] - 182s 517ms/step - loss: 0.4994 - accuracy: 0.8771 - val_loss: 0.5174 - val_accuracy: 0.8752\n",
      "Epoch 125/125\n",
      "352/352 [==============================] - 182s 516ms/step - loss: 0.4962 - accuracy: 0.8778 - val_loss: 0.4677 - val_accuracy: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bbc783e7c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(x_train, y_train, batch_size=128),\n",
    "                   epochs=125, validation_data=(x_vaild, y_valid), callbacks=[checkpointer],\n",
    "                   verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
